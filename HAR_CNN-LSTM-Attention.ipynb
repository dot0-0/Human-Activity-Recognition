{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25309,"status":"ok","timestamp":1701542995983,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"NQ5WkLIn3MQu","outputId":"00d87988-9765-4646-8700-4dfecadd85bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8072,"status":"ok","timestamp":1701543006120,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"Nrgo2bhVFOng"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import torch\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import f1_score, confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3513,"status":"ok","timestamp":1701547360185,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"6cGL6RZVFOnj","outputId":"3b366e72-e596-43a5-eef1-cca2a3963473"},"outputs":[{"name":"stdout","output_type":"stream","text":["(7352, 9, 128) (2947, 9, 128)\n","(7352,) (2947,)\n"]}],"source":["data_dir = '/content/drive/MyDrive/UCI HAR Dataset'\n","output_dir = '/content/uci_data'\n","# Dataset/UCI HAR Dataset/data/train/Inertial Signals/body_acc_x_train.txt\n","# subject_data = np.loadtxt(f'{data_dir}/train/subject_train.txt')\n","# Samples\n","train_acc_x = np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_x_train.txt')\n","train_acc_y = np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_y_train.txt')\n","train_acc_z = np.loadtxt(f'{data_dir}/train/Inertial Signals/body_acc_z_train.txt')\n","train_gyro_x = np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_x_train.txt')\n","train_gyro_y = np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_y_train.txt')\n","train_gyro_z = np.loadtxt(f'{data_dir}/train/Inertial Signals/body_gyro_z_train.txt')\n","train_tot_acc_x = np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_x_train.txt')\n","train_tot_acc_y = np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_y_train.txt')\n","train_tot_acc_z = np.loadtxt(f'{data_dir}/train/Inertial Signals/total_acc_z_train.txt')\n","\n","test_acc_x = np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_x_test.txt')\n","test_acc_y = np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_y_test.txt')\n","test_acc_z = np.loadtxt(f'{data_dir}/test/Inertial Signals/body_acc_z_test.txt')\n","test_gyro_x = np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_x_test.txt')\n","test_gyro_y = np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_y_test.txt')\n","test_gyro_z = np.loadtxt(f'{data_dir}/test/Inertial Signals/body_gyro_z_test.txt')\n","test_tot_acc_x = np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_x_test.txt')\n","test_tot_acc_y = np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_y_test.txt')\n","test_tot_acc_z = np.loadtxt(f'{data_dir}/test/Inertial Signals/total_acc_z_test.txt')\n","\n","# Stacking channels together data\n","train_data = np.stack((train_acc_x, train_acc_y, train_acc_z,\n","                       train_gyro_x, train_gyro_y, train_gyro_z,\n","                       train_tot_acc_x, train_tot_acc_y, train_tot_acc_z), axis=1)\n","X_test = np.stack((test_acc_x, test_acc_y, test_acc_z,\n","                      test_gyro_x, test_gyro_y, test_gyro_z,\n","                      test_tot_acc_x, test_tot_acc_y, test_tot_acc_z), axis=1)\n","# labels\n","train_labels = np.loadtxt(f'{data_dir}/train/y_train.txt')\n","train_labels -= np.min(train_labels)\n","y_test = np.loadtxt(f'{data_dir}/test/y_test.txt')\n","y_test -= np.min(y_test)\n","print(train_data.shape, X_test.shape)\n","print(train_labels.shape, y_test.shape)\n","\n","X_train, X_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n","\n","dat_dict = dict()\n","dat_dict[\"samples\"] = torch.from_numpy(X_train)\n","dat_dict[\"labels\"] = torch.from_numpy(y_train)\n","torch.save(dat_dict, os.path.join(output_dir, \"train.pt\"))\n","\n","dat_dict = dict()\n","dat_dict[\"samples\"] = torch.from_numpy(X_val)\n","dat_dict[\"labels\"] = torch.from_numpy(y_val)\n","torch.save(dat_dict, os.path.join(output_dir, \"val.pt\"))\n","\n","dat_dict = dict()\n","dat_dict[\"samples\"] = torch.from_numpy(X_test)\n","dat_dict[\"labels\"] = torch.from_numpy(y_test)\n","torch.save(dat_dict, os.path.join(output_dir, \"test.pt\"))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1003,"status":"ok","timestamp":1701547397923,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"LJhYy1GVFOnk"},"outputs":[],"source":["data_dir = \"/content/uci_data\"  # Adjust this based on the correct directory path\n","output_dir = \"./output_data\"  # Adjust this based on your desired output directory\n","os.makedirs(output_dir, exist_ok=True)\n","\n","few_lbl_percentages = [1, 5, 10, 50, 75]\n","\n","for percentage in few_lbl_percentages:\n","    data = torch.load(os.path.join(data_dir, \"train_100per.pt\"))  # Load from the correct path\n","    x_data = data[\"samples\"].numpy()\n","    y_data = data[\"labels\"].numpy()\n","\n","    X_train, X_val, y_train, y_val = train_test_split(x_data, y_data, test_size=percentage / 100, random_state=0)\n","\n","    few_shot_dataset = {\"samples\": X_val, \"labels\": y_val}\n","\n","    # Saving data\n","    data_save = dict()\n","    data_save[\"samples\"] = torch.from_numpy(X_val)\n","    data_save[\"labels\"] = torch.from_numpy(y_val)\n","    torch.save(data_save, os.path.join(output_dir, f\"train_{percentage}per.pt\"))\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":458,"status":"ok","timestamp":1701547489164,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"L5xD5mCrFOnk"},"outputs":[],"source":["def format_data_x(datafile):\n","    x_data = None\n","    for item in datafile:\n","        item_data = np.loadtxt(item, dtype=np.float)\n","        if x_data is None:\n","            x_data = np.zeros((len(item_data), 1))\n","        x_data = np.hstack((x_data, item_data))\n","    x_data = x_data[:, 1:]\n","    print(\"1\", x_data.shape) #(7352, 1152)\n","    X = None\n","    for i in range(len(x_data)):\n","        row = np.asarray(x_data[i, :])\n","        # row = row.reshape(9, 128).T\n","        row = row.reshape(9, 128).T\n","        if X is None:\n","            # X = np.zeros((len(x_data), 128, 9))\n","            X = np.zeros((len(x_data), 128, 9))\n","        X[i] = row\n","    print(\"1\", X.shape) #(7352, 128, 3)\n","    return X\n","\n","\n","def format_data_y(datafile):\n","    data = np.loadtxt(datafile, dtype=np.int) - 1\n","    YY = np.eye(6)[data]\n","    return YY\n","\n","\n","# Load data function, if there exists parsed data file, then use it\n","# If not, parse the original dataset from scratch\n","def load_data(data_folder):\n","\n","    if os.path.isfile(data_folder + 'train_100per.pt') == True:\n","        print(\".pt loaded\")\n","        train_data = torch.load(data_folder + 'train_100per.pt')\n","        test_data = torch.load(data_folder + 'test.pt')\n","        X_train = train_data['samples']\n","        Y_train = train_data['labels']\n","        X_test = test_data['samples']\n","        Y_test = test_data['labels']\n","        return X_train, Y_train, X_test, Y_test\n","    else:\n","        # This for processing the dataset from scratch\n","        # After downloading the dataset, put it to somewhere that str_folder can find\n","        str_folder = data_folder\n","        INPUT_SIGNAL_TYPES = [\n","            \"body_acc_x_\",\n","            \"body_acc_y_\",\n","            \"body_acc_z_\",\n","            \"body_gyro_x_\",\n","            \"body_gyro_y_\",\n","            \"body_gyro_z_\",\n","            \"total_acc_x_\",\n","            \"total_acc_y_\",\n","            \"total_acc_z_\"\n","        ]\n","\n","        str_train_files = [str_folder + 'train/' + 'Inertial Signals/' + item + 'train.txt' for item in\n","                           INPUT_SIGNAL_TYPES]\n","        str_test_files = [str_folder + 'test/' + 'Inertial Signals/' +\n","                          item + 'test.txt' for item in INPUT_SIGNAL_TYPES]\n","        str_train_y = str_folder + 'train/y_train.txt'\n","        str_test_y = str_folder + 'test/y_test.txt'\n","\n","        X_train = format_data_x(str_train_files)\n","        X_test = format_data_x(str_test_files)\n","        Y_train = format_data_y(str_train_y)\n","        Y_test = format_data_y(str_test_y)\n","\n","        print(str_folder)\n","    return X_train, onehot_to_label(Y_train), X_test, onehot_to_label(Y_test)\n","\n","\n","\n","\n","def onehot_to_label(y_onehot):\n","    a = np.argwhere(y_onehot == 1)\n","    return a[:, -1]\n","\n","\n","class data_loader(Dataset):\n","    def __init__(self, samples, labels, t):\n","        self.samples = samples\n","        self.labels = labels\n","        self.T = t\n","\n","    def __getitem__(self, index):\n","        sample, target = self.samples[index], self.labels[index]\n","        if self.T:\n","            return self.T(sample), target\n","        else:\n","            return sample, target\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","\n","def normalize(x):\n","    x_min = x.min(axis=(0, 2, 3), keepdims=True)\n","    x_max = x.max(axis=(0, 2, 3), keepdims=True)\n","    x_norm = (x - x_min) / (x_max - x_min)\n","    return x_norm\n","\n","\n","def load(data_folder, batch_size):\n","    x_train, y_train, x_test, y_test = load_data(data_folder)\n","    transform = None\n","    train_set = data_loader(x_train, y_train, transform)\n","    test_set = data_loader(x_test, y_test, transform)\n","    train_loader = DataLoader(\n","        train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","# data_folder = '/Dataset/'\n","# load(data_folder, 64)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Attention blocks\n","Reference: Learn To Pay Attention\n","\"\"\"\n","class ProjectorBlock(nn.Module):\n","    def __init__(self, in_features, out_features):\n","        super(ProjectorBlock, self).__init__()\n","        self.op = nn.Conv1d(in_channels=in_features, out_channels=out_features,\n","            kernel_size=1, padding=0, bias=False)\n","\n","    def forward(self, x):\n","        return self.op(x)\n","\n","\n","class SpatialAttn(nn.Module):\n","    def __init__(self, in_features, normalize_attn=True):\n","        super(SpatialAttn, self).__init__()\n","        self.normalize_attn = normalize_attn\n","        self.op = nn.Conv2d(in_channels=in_features, out_channels=1,\n","            kernel_size=1, padding=0, bias=False)\n","\n","    def forward(self, l, g):\n","        N, C, H, W = l.size()\n","        c = self.op(l+g) # (batch_size,1,H,W)\n","        if self.normalize_attn:\n","            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,H,W)\n","        else:\n","            a = torch.sigmoid(c)\n","        g = torch.mul(a.expand_as(l), l)\n","        if self.normalize_attn:\n","            g = g.view(N,C,-1).sum(dim=2) # (batch_size,C)\n","        else:\n","            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n","        return c.view(N,1,H,W), g\n","\n","\"\"\"\n","Temporal attention block\n","Reference: https://github.com/philipperemy/keras-attention-mechanism\n","\"\"\"\n","class TemporalAttn(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(TemporalAttn, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n","        self.fc2 = nn.Linear(self.hidden_size*2, self.hidden_size, bias=False)\n","\n","    def forward(self, hidden_states):\n","        # (batch_size, time_steps, hidden_size)\n","        score_first_part = self.fc1(hidden_states)\n","        # (batch_size, hidden_size)\n","        h_t = hidden_states[:,-1,:]\n","        # (batch_size, time_steps)\n","        score = torch.bmm(score_first_part, h_t.unsqueeze(2)).squeeze(2)\n","        attention_weights = F.softmax(score, dim=1)\n","        # (batch_size, hidden_size)\n","        # context_vector = torch.bmm(hidden_states.permute(0,2,1), attention_weights.unsqueeze(2)).squeeze(2)\\\n","        context_vector = torch.bmm(hidden_states.transpose(1, 2), attention_weights.unsqueeze(2)).squeeze(2)\n","        # (batch_size, hidden_size*2)\n","        pre_activation = torch.cat((context_vector, h_t), dim=1)\n","        # (batch_size, hidden_size)\n","        attention_vector = self.fc2(pre_activation)\n","        attention_vector = torch.tanh(attention_vector)\n","\n","        return attention_vector, attention_weights\n","\n","# Test\n","if __name__ == '__main__':\n","    # 2d block\n","    # spatial_block = SpatialAttn(in_features=3)\n","    # l = torch.randn(16, 3, 128, 128)\n","    # g = torch.randn(16, 3, 128, 128)\n","    # print(spatial_block(l, g))\n","    # temporal block\n","    temporal_block = TemporalAttn(hidden_size=256)\n","    x = torch.randn(16, 30, 256)\n","    attention_vector, attention_weights = temporal_block(x)\n","    # print(attention_vector.shape) # should output (16, 256)\n","    # print(attention_weights.shape) # should output (16, 30)\n","    # print(temporal_block(x).shape)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":321,"status":"ok","timestamp":1701547494293,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"ygR-Ye7LFOnl"},"outputs":[],"source":["class Network(nn.Module):\n","    \n","    def __init__(self):\n","        super(Network, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(in_channels = 9, out_channels = 64, kernel_size=6, stride=1, padding=2),            \n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size=3, stride=1, padding =2),\n","            nn.ReLU(),\n","            nn.MaxPool1d(kernel_size=2)\n","        )\n","\n","        self.flatten = torch.nn.Flatten()\n","        self.dropout = torch.nn.Dropout(0.1)\n","\n","        self.lstm = nn.LSTM(input_size=32, hidden_size=128, num_layers=1)\n","        self.tanh = torch.nn.Tanh()\n","        self.attn = TemporalAttn(hidden_size=128)\n","        self.fc = nn.Linear(in_features=128, out_features=6)\n","        self.softmax = torch.nn.Softmax(dim = 1)\n","\n","\n","    def forward(self, x):\n","        # print(\"1\", x.dtype, x.shape)\n","        # x = x.permute(0,2,1)\n","        out = self.conv1(x)\n","        # print(\"c1\", out.dtype, out.shape)\n","        out = self.conv2(out)\n","        # print(\"c2\", out.dtype, out.shape)\n","        out = self.dropout(out)\n","        # print(\"dropout\", out.shape, out.dtype)\n","        out, hidden = self.lstm(out)\n","        out = self.tanh(out)\n","        # print(\"lstm\", out.dtype, out.shape)\n","        #attention\n","        out, weights=self.attn(out)\n","        out = self.flatten(out)\n","        # print(\"flatten\", out.dtype, out.shape)\n","        out = self.fc(out)\n","        # print(\"fc\", out.dtype, out.shape)\n","        out = self.softmax(out)\n","        # print(\"sm\", out.dtype, out.shape)\n","\n","        return out"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":355,"status":"ok","timestamp":1701547499140,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"tscW2yJ0FOnl"},"outputs":[],"source":["DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","data_folder = './uci_data/'\n","result = []\n","f1_result = []\n","classes = ['WALKING', 'WALKING_UPSTAIRS',  'WALKING_DOWNSTAIRS','SITTING', 'STANDING', 'LAYING']\n","\n","result_path='//output//'\n","testAcc_csv=result_path+'result_cnn-lstm_UCI.csv'\n","f1_csv=result_path+'result_f1_cnn-lstm_UCI.csv'\n","confusion_img=result_path+'confusion matrix_cnn-lstm_UCI.png'\n","plot_img=result_path+'plot_cnn-lstm_UCI.png'"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701547507493,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"sjW8yyLMFOnl"},"outputs":[],"source":["def load_data_interface():\n","    train_loader, test_loader = load(\n","        data_folder, batch_size=64)  # Ensure batch_size is defined\n","    return train_loader, test_loader\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":415,"status":"ok","timestamp":1701547520602,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"nizKsXpoFOnm"},"outputs":[],"source":["def calculate_accuracy(predictions, labels):\n","    correct = (np.array(predictions) == np.array(labels)).sum()\n","    total = len(predictions)\n","    return (correct / total) * 100\n","\n","def calculate_f1_scores(predictions, labels):\n","    f1_scores = f1_score(labels, predictions, average=None)\n","    avg_f1_score = np.mean(f1_scores)\n","    return avg_f1_score\n","\n","def generate_confusion_matrix(predictions, labels):\n","    cm = confusion_matrix(labels, predictions)\n","    return cm"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701547523079,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"WBM80LktFOnm"},"outputs":[],"source":["def evaluate_model(model, test_loader):\n","    model.eval()\n","    criterion = nn.CrossEntropyLoss()\n","    all_predictions = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for samples, labels in test_loader:\n","            samples, labels = samples.to(DEVICE).float(), labels.to(DEVICE).long()\n","            outputs = model(samples)\n","            loss = criterion(outputs, labels)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_predictions.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = calculate_accuracy(all_predictions, all_labels)\n","    f1_scores = calculate_f1_scores(all_predictions, all_labels)\n","    confusion_matrix = generate_confusion_matrix(all_predictions, all_labels)\n","\n","    np.savetxt(testAcc_csv, np.array([accuracy]), delimiter=',', fmt='%.4f')\n","    np.savetxt(f1_csv, f1_scores.reshape(1, -1), delimiter=',', fmt='%.4f')\n","    np.savetxt(confusion_img, confusion_matrix, delimiter=',', fmt='%d')\n","\n","    return accuracy, f1_scores, confusion_matrix\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1701547857975,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"dPZplN3WFOnn"},"outputs":[],"source":["def train_model(model, optimizer, train_loader, test_loader, n_epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    result = []  # To store training results for plotting or analysis\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        correct = 0\n","        total_loss = 0\n","\n","        for sample, label in train_loader:\n","            sample, label = sample.to(DEVICE).float(), label.to(DEVICE).long()\n","            optimizer.zero_grad()\n","            output = model(sample)\n","            loss = criterion(output, label)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(output.data, 1)\n","            correct += (predicted == label).sum().item()\n","\n","        train_accuracy = 100 * correct / len(train_loader.dataset)\n","        avg_loss = total_loss / len(train_loader)\n","\n","        # Validate the model on the test set after each epoch\n","        test_accuracy, _, _ = evaluate_model(model, test_loader)  # Assuming accuracy is the first value returned\n","\n","        # Store the epoch-wise results for analysis or plotting\n","        result.append([train_accuracy, test_accuracy, avg_loss])\n","\n","        print(f'Epoch [{epoch+1}/{n_epochs}] - '\n","              f'Train Accuracy: {float(train_accuracy):.2f}% - '\n","              f'Test Accuracy: {float(test_accuracy):.2f}% - '\n","              f'Average Loss: {float(avg_loss):.4f}')\n","        \n","    torch.save(model.state_dict(), 'trained_model.pth')\n","    return result  # Return the training results for later use (e.g., plotting)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33180,"status":"ok","timestamp":1701548932170,"user":{"displayName":"Shashank Bharadwaj","userId":"17440477624314309050"},"user_tz":-330},"id":"IDjq_TuOFOnn","outputId":"36d736b0-5a4e-4bf5-d4c6-f9c35afc4ddb"},"outputs":[],"source":["train_loader, test_loader = load_data_interface()\n","model = Network().to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","result = train_model(model, optimizer, train_loader, test_loader, 30)\n","accuracy, f1_scores, confusion_matrix = evaluate_model(model, test_loader, testAcc_csv, f1_csv, confusion_img)\n","# Calculate averages or other post-processing steps"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
